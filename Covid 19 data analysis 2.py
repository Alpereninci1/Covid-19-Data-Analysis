# -*- coding: utf-8 -*-
"""data_mining_hw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZmibkDiShF2KSdksBO5R-7Y-H-s67RCB
"""

import pandas_profiling
import numpy as np
from numpy import percentile
from numpy.random import rand
import matplotlib.pyplot as plt
import pandas as pd
from scipy.spatial import distance

#https://www.google.com/covid19/mobility/
url = 'https://drive.google.com/file/d/18gyHbx6rfogq3yQ-GR9COjcGgyYlCnBZ/view?usp=sharing'
url2020 = 'https://drive.google.com/uc?id=' + url.split('/')[-2]
url = 'https://drive.google.com/file/d/1Eg8Lffm49bc-bGFkv_4ddrQw8U8WE6P4/view?usp=sharing'
url2021 = 'https://drive.google.com/uc?id=' + url.split('/')[-2]

df20 = pd.read_csv(url2020)
df20.info()

df21 = pd.read_csv(url2021)
df21.info()

"""Soru 1"""

bos_sutun = [col for col in df20.columns if df20[col].isnull().all()]

df20.drop(bos_sutun,axis=1,inplace=True)
  
display(df20)

bos_sutun2=[col for col in df21.columns if df21[col].isnull().all()]

df21.drop(bos_sutun2,axis=1,inplace=True)

display(df21)

"""Soru 2 """

#soru2
import pandas_profiling
import numpy as np
from numpy import percentile
from numpy.random import rand
import matplotlib.pyplot as plt
import pandas as pd
from scipy.spatial import distance

#https://www.google.com/covid19/mobility/
url = 'https://drive.google.com/file/d/18gyHbx6rfogq3yQ-GR9COjcGgyYlCnBZ/view?usp=sharing'
url2020 = 'https://drive.google.com/uc?id=' + url.split('/')[-2]
url = 'https://drive.google.com/file/d/1Eg8Lffm49bc-bGFkv_4ddrQw8U8WE6P4/view?usp=sharing'
url2021 = 'https://drive.google.com/uc?id=' + url.split('/')[-2]

df20 = pd.read_csv(url2020)
df21 = pd.read_csv(url2021)
df20.drop('metro_area',inplace=True, axis=1)
df20.drop('census_fips_code',inplace=True, axis=1)
df20.date = pd.to_datetime(df20.date)

da=df20.groupby(pd.Grouper(key='date', freq='1M')).mean()


cov = np.cov(da.values.T)
inv_covmat = np.linalg.inv(cov)
array_1=df20[['retail_and_recreation_percent_change_from_baseline', 'grocery_and_pharmacy_percent_change_from_baseline', 
              'parks_percent_change_from_baseline','transit_stations_percent_change_from_baseline',
              'workplaces_percent_change_from_baseline','residential_percent_change_from_baseline']].to_numpy()

print(array_1)
array_2=da.to_numpy()

print(array_2)


for i in range(11):
    for j in range(11):
      db = distance.mahalanobis(array_1[i],array_2[j],inv_covmat)
      display(db)

"""Soru 3

"""

#soru3
import pandas_profiling
import numpy as np
from numpy import percentile
from numpy.random import rand
import matplotlib.pyplot as plt
import pandas as pd
from scipy.spatial import distance

#https://www.google.com/covid19/mobility/
url = 'https://drive.google.com/file/d/18gyHbx6rfogq3yQ-GR9COjcGgyYlCnBZ/view?usp=sharing'
url2020 = 'https://drive.google.com/uc?id=' + url.split('/')[-2]
url = 'https://drive.google.com/file/d/1Eg8Lffm49bc-bGFkv_4ddrQw8U8WE6P4/view?usp=sharing'
url2021 = 'https://drive.google.com/uc?id=' + url.split('/')[-2]

df20 = pd.read_csv(url2020)
df21 = pd.read_csv(url2021)


df21.drop('metro_area',inplace=True, axis=1)
df21.drop('census_fips_code',inplace=True, axis=1)
df20.date = pd.to_datetime(df20.date)
g1=df20.groupby(pd.Grouper(key='date', freq='M')).mean()
g1 = g1.iloc[1:]

df21.date = pd.to_datetime(df21.date)
g2=df21.groupby(pd.Grouper(key='date', freq='M')).mean()



def Euclidean_Dist(df1, df2, cols=['retail_and_recreation_percent_change_from_baseline','grocery_and_pharmacy_percent_change_from_baseline',
                                   'parks_percent_change_from_baseline','transit_stations_percent_change_from_baseline',
                                   'workplaces_percent_change_from_baseline','residential_percent_change_from_baseline']):
    return np.linalg.norm(df1[cols].values - df2[cols].values,axis=1)

print(Euclidean_Dist(g1,g2))

"""Soru 4

"""

import matplotlib.pyplot as plt
df20.drop('metro_area',inplace=True, axis=1)
df20.drop('census_fips_code',inplace=True, axis=1)
df20.date = pd.to_datetime(df20.date)
g1=df20.groupby(pd.Grouper(key='date', freq='M')).mean()

arr= []
for i in range(1,6):
  df=df20.sample(i*50).groupby(pd.Grouper(key='date', freq='M')).mean()
  df_abs=abs(g1-df).sum(axis=1).sum(axis=0)
  arr.append(df_abs)

print(arr)
arr1=np.array([50,100,150,200,250])
x=arr
y=arr1
# plotting
plt.title("Line graph")
plt.xlabel("X axis")
plt.ylabel("Y axis")
plt.plot(x, y, color ="red")
plt.show()

"""Soru 5"""

#soru5
df20.date = pd.to_datetime(df20.date)
g1=df20.groupby(pd.Grouper(key='date', freq='M')).mean()


df1=df20.groupby(pd.Grouper(key='date', freq='M')).sample(50).groupby(pd.Grouper(key='date',freq='M')).mean()
df2=df20.groupby(pd.Grouper(key='date', freq='M')).sample(100).groupby(pd.Grouper(key='date',freq='M')).mean()
df3=df20.groupby(pd.Grouper(key='date', freq='M')).sample(150).groupby(pd.Grouper(key='date',freq='M')).mean()
df4=df20.groupby(pd.Grouper(key='date', freq='M')).sample(200).groupby(pd.Grouper(key='date',freq='M')).mean()
df5=df20.groupby(pd.Grouper(key='date', freq='M')).sample(300).groupby(pd.Grouper(key='date',freq='M')).mean()

df_abs1=abs(g1-df1).sum(axis=1).sum(axis=0)
df_abs2=abs(g1-df2).sum(axis=1).sum(axis=0)
df_abs3=abs(g1-df3).sum(axis=1).sum(axis=0)
df_abs4=abs(g1-df4).sum(axis=1).sum(axis=0)
df_abs5=abs(g1-df5).sum(axis=1).sum(axis=0)

mutlak_deger_arr=np.array([df_abs1,df_abs2,df_abs3,df_abs4,df_abs5])
sample_arr=np.array([50,100,150,200,300])
print(mutlak_deger_arr)

x=sample_arr 
y=mutlak_deger_arr
plt.title("Line graph")
plt.xlabel("X axis")
plt.ylabel("Y axis")
plt.plot(x, y, color ="green")
plt.show()

"""### Yapılacaklar
1. Yukarıdaki data framede içerisinde data olmayan (tüm sütun null), sütunları çıkarınız. 
2. Mahalanobis distance distributiondan uzaklığı ölçtüğü için outlier belirlenmesinde kullanılabilir.2020 ve 2021 her iki datayıda aylara göre gruplandırdıktan(mean kullanabilirsiniz) sonra (aggregation) her bir satır ile data (tüm sütun) arasındaki Mahalanobis distance'ı hesaplayarak yeni bir sütun olarak ekleyiniz ve buradaki en büyük elemanın outlier olduğunu 2020 ve 2021 yılları için ayrı ayrı gösteriniz.

3. 2020 ve 2021 datalarını aylara göre grupladıktan sonra (mean değerleri ile) en az iki adet fark/benzerlik ölçümü (**slaylatlardaki similarity measures**) kullanarak 2020 ve 2021 datalarının 9-14 sütun verilerinin **aynı aylarda** birbirlerine ne oranda benzediğini bulunuz.
4. 2020 datasından (50-1000) aralığında farklı büyüklüklerde samplelar oluşturarak aylık mean değerlerin ortalama ne kadar değiştiğini grafikle gösteriniz (x sample size, y ortalama değişim): **Açıklama**▶
Tüm datanın aylık ortalama değerleri ile sample datanın aylık ortalama değerleri arasındaki farkların mutlak değerlerini toplayarak ortalamasını almanız gerekiyor. Bu şekilde her bir sample için bir hata datası elde etmiş oluyorsunuz. Sonra bunları x axisde sample size, y axisde hata olacak şekilde grafiklemeniz istenmektedir.
5. 2020 datasından her aydan (50-1000) aralığında olacak şekilde samplelar oluşturarak aylık mean değerlerin ortalama ne kadar değiştiğini grafikle gösteriniz (x sample size, y ortalama değişim): **Açıklama**▶ Bu soruda her bir aydan eşit miktarda sampller alarak sample oluşturmanız (mesela 50 için her bir aydan 50şer satır alarak, aysayısı x 50 büyüklüğünde bir sample elde etmiş oluyorsunuz) ve 4.sorudaki gibi ortalama hatayı bularak yine sample size'a göre grafiklemeniz istenmektedir.

### Teslim
Her sorunun altında **hem kodu ve hemde çıktısını** içeren Jupiter notebook dökümanını **pdfe** çevirerek classroom üzerinden teslim ediniz(colab de direk print ile pdf alabilirsiniz).
Çıktının güzel görünmesi için latex çıktı alarak oradan düzenleyebilirsiniz. Yada html aldıktan sonra print yapabilirsiniz.
"""